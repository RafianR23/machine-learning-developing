# -*- coding: utf-8 -*-
"""NLP_Submission_RafianRamadhani.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ymBX_dE5sFJv_HTysw8E_X6ESNrpmOVn

## Import Dataset dari Folder di Google Drive
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
df=pd.read_csv('/content/gdrive/My Drive/Startup/DicodingMLPemula/Submission_NLP/dataset_film/Train.csv')
#df_test=pd.read_csv('/content/gdrive/My Drive/Sanbercode/Week4/Project/test.csv')
#df_test=pd.read_csv('/content/gdrive/My Drive/Sanbercode/Week4/Project/test.csv')
#df_test=pd.read_csv('/content/gdrive/My Drive/Sanbercode/Week4/Project/test.csv')

"""## Standarisasi Nilai"""

sns.set_context('paper')

SEED = 42
# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 3000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 300
# This is fixed.
EMBEDDING_DIM = 100

"""## Mengambil 3.000 dari 40.000 Data Untuk di Uji"""

df=df.head(3000)

"""## Langkah Tokenize Text"""

token = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS)
token.fit_on_texts(df['text'])
x_encoded = token.texts_to_sequences(df['text'])
x = tf.keras.preprocessing.sequence.pad_sequences(x_encoded, maxlen=MAX_SEQUENCE_LENGTH)
y = df['label']
print("X: {}\nY: {}".format(len(x), len(y)))

"""## Membagi Daat Latih dan Uji"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=SEED)

"""## Menyusun Arsitektur dari Tensorflow"""

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=x.shape[1]))
model.add(tf.keras.layers.SpatialDropout1D(0.2))
model.add(tf.keras.layers.LSTM(128, dropout=0.1, recurrent_dropout=0.1, return_sequences=True))
model.add(tf.keras.layers.LSTM(64, dropout=0.1, recurrent_dropout=0.1))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

"""## Melakukan Training pada Data"""

hist = model.fit(x, y, epochs=10, validation_data=(x_test, y_test))

"""## Membuat Plot dari History"""

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()